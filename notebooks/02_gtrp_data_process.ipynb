{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os, sys\n",
    "\n",
    "def test_dir(dir):\n",
    "    if not(os.path.isdir(dir)):\n",
    "        os.mkdir(dir)\n",
    "    return os.path.isdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.gitkeep', 'noaa_global', 'tech_challenge', 'temp_change', 'wbpy']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('..//data//raw')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data from tech challenge\n",
    "\n",
    "- [Dados da Vitivinicultura](http://vitibrasil.cnpuv.embrapa.br/index.php?opcao=opt_01) was indicated as main source of data;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw > interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando dataframe das exportações\n",
    "\n",
    "#0. Create folder\n",
    "test_dir('..//data//interim//tech_challenge//')\n",
    "\n",
    "df_exp = pd.read_csv('..//data//raw//tech_challenge//ExpVinho.csv', sep=';')\n",
    "df_exp = df_exp.melt(id_vars=['Id','País'])\n",
    "\n",
    "vars = df_exp.variable.unique()\n",
    "valor = [x for x in vars if x.endswith('.1')]\n",
    "quantidade = list(set(vars) - set(valor))\n",
    "\n",
    "df_quant = df_exp.loc[df_exp['variable'].isin(quantidade)]\\\n",
    "    .rename(columns={\n",
    "          'value':'quantidade_exportada_pais'\n",
    "        , 'variable':'ano'\n",
    "        , 'País':'pais'\n",
    "        , 'Id':'id'\n",
    "        })\n",
    "\n",
    "df_quant['ano'] = df_quant['ano'].astype(int)\n",
    "\n",
    "df_value = df_exp.loc[df_exp['variable'].isin(valor)]\\\n",
    "    .rename(columns={\n",
    "          'value':'valor_exportado_pais'\n",
    "        , 'variable':'ano'\n",
    "        , 'País':'pais'\n",
    "        , 'Id':'id'\n",
    "        })\n",
    "df_value['ano'] = [int(x.replace('.1', '')) for x in df_value['ano']]\n",
    "\n",
    "df_exp = pd.merge(df_quant, df_value, on=['id','pais','ano'])[['id','pais','ano','quantidade_exportada_pais','valor_exportado_pais']]\\\n",
    "    .drop(columns=['id'])\n",
    "\n",
    "\n",
    "df_exp.to_csv('..//data//interim//tech_challenge//exportacao_vinhos.csv',index=False, sep=';', decimal=',') # export data to share with the project group members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando df das comercializações no RS\n",
    "\n",
    "#0. Create folder\n",
    "test_dir('..//data//interim//tech_challenge//')\n",
    "\n",
    "df_com = pd.read_csv('..//data//raw//tech_challenge//Comercio.csv', sep=';', header=None)\n",
    "lista_anos = list(df_exp['ano'].unique())\n",
    "df_com.columns = ['id','id_produto','produto'] + lista_anos\n",
    "\n",
    "df_com = df_com\\\n",
    "    .melt(id_vars=['id','id_produto','produto'])\\\n",
    "    .rename(columns={\n",
    "          'variable':'ano'\n",
    "        , 'value':'quantidade_com_rs'\n",
    "    })\\\n",
    "    .drop(columns=['id'])\n",
    "\n",
    "df_com['id_produto'] = [str(x).strip().lower() for x in df_com['id_produto']]\n",
    "df_com['produto'] = [str(x).strip().lower() for x in df_com['produto']]\n",
    "df_com['ano'] = df_com['ano'].astype(int)\n",
    "\n",
    "df_com.to_csv('..//data//interim//tech_challenge//comercio_vinhos_rs.csv',index=False, sep=';', decimal=',') # export data to share with the project group members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando df das produções no RS\n",
    "\n",
    "#0. Create folder\n",
    "test_dir('..//data//interim//tech_challenge//')\n",
    "\n",
    "df_prod = pd.read_csv('..//data//raw//tech_challenge//Producao.csv', sep=';', header=None)\n",
    "lista_anos = list(df_exp['ano'].unique())\n",
    "df_prod.columns = ['id','id_produto','produto'] + lista_anos\n",
    "\n",
    "df_prod = df_prod\\\n",
    "    .melt(id_vars=['id','id_produto','produto'])\\\n",
    "    .rename(columns={\n",
    "          'variable':'ano'\n",
    "        , 'value':'quantidade_prod_rs'\n",
    "    })\\\n",
    "    .drop(columns=['id'])\n",
    "\n",
    "df_prod['id_produto'] = [str(x).strip().lower() for x in df_prod['id_produto']]\n",
    "df_prod['produto'] = [str(x).strip().lower() for x in df_prod['produto']]\n",
    "df_prod['ano'] = df_prod['ano'].astype(int)\n",
    "\n",
    "df_prod.to_csv('..//data//interim//tech_challenge//producao_vinhos_rs.csv',index=False, sep=';', decimal=',') # export data to share with the project group members"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interim > processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interim to processed\n",
    "\n",
    "#0. Create folder\n",
    "test_dir('..//data//processed//tech_challenge//')\n",
    "\n",
    "df_exp = pd.read_csv('..//data//interim//tech_challenge//exportacao_vinhos.csv', sep=';', decimal=',')\n",
    "df_prod = pd.read_csv('..//data//interim//tech_challenge//producao_vinhos_rs.csv', sep=';', decimal=',')\n",
    "df_com = pd.read_csv('..//data//interim//tech_challenge//comercio_vinhos_rs.csv', sep=';', decimal=',')\n",
    "\n",
    "df_prod_com = df_prod.merge(df_com, on=['id_produto','produto','ano'], how='outer')\n",
    "\n",
    "df_final = df_exp.merge(df_prod_com, on='ano', how='outer')\n",
    "\n",
    "df_final.to_csv('..//data//processed//tech_challenge//df_vinhos.csv',index=False, sep=';', decimal=',') # export data to share with the project group members"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Temperature Change over years](https://www.kaggle.com/datasets/sevgisarac/temperature-change)\n",
    "\n",
    "### raw > processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar output da mudança de temperatura\n",
    "\n",
    "#0. Create folder\n",
    "test_dir('..//data//processed//temp_change//')\n",
    "\n",
    "df = pd.read_csv(\"..//data//raw//temp_change//Environment_Temperature_change_E_All_Data_NOFLAG.csv\", encoding='latin-1') # csv file is encoding as latin-1 type\n",
    "df_countrycode=pd.read_csv('..//data//raw//temp_change//FAOSTAT_data_11-24-2020.csv') #this csv file includes ISO-3 Country Code, this mentioned in Data Wrangling \n",
    "\n",
    "#1. Renaming\n",
    "df.rename(columns = {'Area':'Country Name'},inplace = True)\n",
    "df.set_index('Months', inplace=True)\n",
    "df.rename({'Dec\\x96Jan\\x96Feb': 'Winter', 'Mar\\x96Apr\\x96May': 'Spring', 'Jun\\x96Jul\\x96Aug':'Summer','Sep\\x96Oct\\x96Nov':'Fall'}, axis='index',inplace = True)\n",
    "df.reset_index(inplace = True)\n",
    "\n",
    "#2. Filtering \n",
    "df = df[df['Element'] == 'Temperature change']\n",
    "\n",
    "#2. Drop unwanted columns from df_countrycode\n",
    "df_countrycode.drop(['Country Code','M49 Code','ISO2 Code','Start Year','End Year'],axis=1,inplace=True)\n",
    "df_countrycode.rename(columns = {'Country':'Country Name','ISO3 Code':'Country Code'},inplace=True)\n",
    "\n",
    "#3. Merging with df to df_country\n",
    "df = pd.merge(df, df_countrycode, how='outer', on='Country Name')\n",
    "\n",
    "#2. Drop unwanted columns\n",
    "df.drop(['Area Code','Months Code','Element Code','Element','Unit'],axis=1,inplace=True)\n",
    "\n",
    "#3.Channing dataframe organization\n",
    "df = df.melt(id_vars=[\"Country Code\", \"Country Name\",\"Months\",], var_name=\"year\", value_name=\"tem_change\")\n",
    "df[\"year\"] = [i.split(\"Y\")[-1] for i in df.year]\n",
    "\n",
    "df = df[df['Months']=='Meteorological year']# chose just year data\n",
    "df.drop(['Months'],axis=1,inplace=True) # dropped Months column\n",
    "df.to_csv('..//data//processed//temp_change//temperature_change_Data.csv',index=False, sep=';', decimal=',') # export data to share with the project group members"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NOAA global data](https://www.kaggle.com/datasets/noaa/noaa-global-historical-climatology-network-daily)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw > interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "base_path = '..\\\\data\\\\raw\\\\noaa_global'\n",
    "dest_path = '..\\\\data\\\\interim\\\\noaa_global'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dir(dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_year(file_year='2019'):\n",
    "    base_path = '..\\\\data\\\\raw\\\\noaa_global'\n",
    "    dest_path = '..\\\\data\\\\interim\\\\noaa_global'\n",
    "    if not os.path.exists(f\"{dest_path}\\\\{file_year}.csv\"):\n",
    "\n",
    "        start = datetime.datetime.now()\n",
    "\n",
    "        raw_df = pd.read_csv(f\"{base_path}\\\\ghcnd_all_years\\\\{file_year}.csv.gz\",\n",
    "                        usecols=[0,1,2,3], \n",
    "                        names=['station_id','date', 'stat', 'value'], \n",
    "                        dtype= {\n",
    "                              'station_id' : str\n",
    "                            , 'date': str\n",
    "                            , 'stat': str\n",
    "                            , 'value': np.int16\n",
    "                            },\n",
    "                        engine='c'\n",
    "                        )\n",
    "        raw_df['year'] = np.int16(file_year)\n",
    "        grouped = raw_df.groupby([\"station_id\", 'year',\"stat\"]).mean().reset_index()\n",
    "        grouped = grouped[['year', 'station_id', 'stat', 'value']]\n",
    "        #return grouped\n",
    "        grouped.to_csv(f\"{dest_path}\\\\{file_year}.csv\", index=False)\n",
    "        duration = (datetime.datetime.now() - start).seconds\n",
    "        print(f\"{file_year} took {round(duration/60, 2)} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [year[:4] for year in os.listdir(f\"{base_path}\\\\ghcnd_all_years\\\\\")]\n",
    "years.sort()\n",
    "level_of_parallelism = mp.cpu_count()\n",
    "pool = mp.Pool(level_of_parallelism)\n",
    "pool.map(process_year, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations = pd.read_csv(base_path+'\\\\ghcnd-stations.txt', sep=';', decimal='.')\n",
    "for col in ['id','name']:\n",
    "    df_stations[col] = [x.strip() for x in df_stations[col]]\n",
    "df_stations.to_csv(dest_path+'\\\\df_stations.csv',index=False, sep=';', decimal=',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interim > processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '..\\\\data\\\\interim\\\\noaa_global\\\\'\n",
    "dest_path = '..\\\\data\\\\processed\\\\noaa_global\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [base_path + x for x in os.listdir(base_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot safely convert passed user dtype of int16 for float64 dtyped data in column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge\\.venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1129\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot cast array data from dtype('float64') to dtype('int16') according to the rule 'safe'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_noaa \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(files[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m      2\u001b[0m                       dtype\u001b[39m=\u001b[39;49m {\n\u001b[0;32m      3\u001b[0m                               \u001b[39m'\u001b[39;49m\u001b[39mstation_id\u001b[39;49m\u001b[39m'\u001b[39;49m : \u001b[39mstr\u001b[39;49m\n\u001b[0;32m      4\u001b[0m                             , \u001b[39m'\u001b[39;49m\u001b[39mdate\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mstr\u001b[39;49m\n\u001b[0;32m      5\u001b[0m                             , \u001b[39m'\u001b[39;49m\u001b[39mstat\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mstr\u001b[39;49m\n\u001b[0;32m      6\u001b[0m                             , \u001b[39m'\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m'\u001b[39;49m: np\u001b[39m.\u001b[39;49mint16\n\u001b[0;32m      7\u001b[0m                             },\n\u001b[0;32m      8\u001b[0m                         engine\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mc\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[0;32m      9\u001b[0m                       )\n",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    582\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1697\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1698\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1699\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     (\n\u001b[0;32m   1701\u001b[0m         index,\n\u001b[0;32m   1702\u001b[0m         columns,\n\u001b[0;32m   1703\u001b[0m         col_dict,\n\u001b[1;32m-> 1704\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1705\u001b[0m         nrows\n\u001b[0;32m   1706\u001b[0m     )\n\u001b[0;32m   1707\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1708\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge\\.venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:812\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge\\.venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:889\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge\\.venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1034\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge\\.venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1137\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot safely convert passed user dtype of int16 for float64 dtyped data in column 3"
     ]
    }
   ],
   "source": [
    "df_noaa = pd.read_csv(base_path+\"*\",\n",
    "                      dtype= {\n",
    "                              'station_id' : str\n",
    "                            , 'date': str\n",
    "                            , 'stat': str\n",
    "                            , 'value_min' : np.float64\n",
    "                            , 'value_mean' : np.float64\n",
    "                            , 'value_median' : np.float64\n",
    "                            , 'value_max' : np.float64\n",
    "                            },\n",
    "                        engine='c'\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station_id</th>\n",
       "      <th>stat</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1763</td>\n",
       "      <td>ITE00100554</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>147.873973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1763</td>\n",
       "      <td>ITE00100554</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>100.657534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year   station_id  stat       value\n",
       "0  1763  ITE00100554  TMAX  147.873973\n",
       "1  1763  ITE00100554  TMIN  100.657534"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noaa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interim > processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "\n",
    "base_path = '..\\\\data\\\\interim\\\\noaa_global\\\\'\n",
    "\n",
    "def read_csv(args):\n",
    "    return pd.read_csv(args, sep=';', decimal='.')\n",
    "\n",
    "df_noaa = pd.concat(map(read_csv, glob.glob(base_path+'years\\\\*.csv')))\n",
    "\n",
    "df_stations = pd.read_csv(base_path+'df_stations.csv', sep=';', decimal=',').rename(columns={'id':'station_id'})\n",
    "\n",
    "df_noaa = df_noaa.merge(df_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>year</th>\n",
       "      <th>stat</th>\n",
       "      <th>value_min</th>\n",
       "      <th>value_mean</th>\n",
       "      <th>value_median</th>\n",
       "      <th>value_max</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ITE00100554</td>\n",
       "      <td>1763</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>-39</td>\n",
       "      <td>147.873973</td>\n",
       "      <td>147.0</td>\n",
       "      <td>309</td>\n",
       "      <td>45.4717</td>\n",
       "      <td>9.1892</td>\n",
       "      <td>150.0</td>\n",
       "      <td>MILAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ITE00100554</td>\n",
       "      <td>1763</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>-63</td>\n",
       "      <td>100.657534</td>\n",
       "      <td>99.0</td>\n",
       "      <td>233</td>\n",
       "      <td>45.4717</td>\n",
       "      <td>9.1892</td>\n",
       "      <td>150.0</td>\n",
       "      <td>MILAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ITE00100554</td>\n",
       "      <td>1764</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>20</td>\n",
       "      <td>153.592896</td>\n",
       "      <td>148.5</td>\n",
       "      <td>301</td>\n",
       "      <td>45.4717</td>\n",
       "      <td>9.1892</td>\n",
       "      <td>150.0</td>\n",
       "      <td>MILAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ITE00100554</td>\n",
       "      <td>1764</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>-19</td>\n",
       "      <td>106.581967</td>\n",
       "      <td>93.0</td>\n",
       "      <td>236</td>\n",
       "      <td>45.4717</td>\n",
       "      <td>9.1892</td>\n",
       "      <td>150.0</td>\n",
       "      <td>MILAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ITE00100554</td>\n",
       "      <td>1765</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>-14</td>\n",
       "      <td>149.994521</td>\n",
       "      <td>157.0</td>\n",
       "      <td>292</td>\n",
       "      <td>45.4717</td>\n",
       "      <td>9.1892</td>\n",
       "      <td>150.0</td>\n",
       "      <td>MILAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12143427</th>\n",
       "      <td>USC00453859</td>\n",
       "      <td>2019</td>\n",
       "      <td>DAPR</td>\n",
       "      <td>3</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47.3872</td>\n",
       "      <td>-121.3925</td>\n",
       "      <td>776.9</td>\n",
       "      <td>WA HYAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12143428</th>\n",
       "      <td>USC00453859</td>\n",
       "      <td>2019</td>\n",
       "      <td>MDPR</td>\n",
       "      <td>38</td>\n",
       "      <td>189.250000</td>\n",
       "      <td>212.0</td>\n",
       "      <td>295</td>\n",
       "      <td>47.3872</td>\n",
       "      <td>-121.3925</td>\n",
       "      <td>776.9</td>\n",
       "      <td>WA HYAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12143429</th>\n",
       "      <td>USC00453859</td>\n",
       "      <td>2019</td>\n",
       "      <td>PRCP</td>\n",
       "      <td>0</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>516</td>\n",
       "      <td>47.3872</td>\n",
       "      <td>-121.3925</td>\n",
       "      <td>776.9</td>\n",
       "      <td>WA HYAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12143430</th>\n",
       "      <td>USC00453859</td>\n",
       "      <td>2019</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>0</td>\n",
       "      <td>103.526316</td>\n",
       "      <td>10.0</td>\n",
       "      <td>711</td>\n",
       "      <td>47.3872</td>\n",
       "      <td>-121.3925</td>\n",
       "      <td>776.9</td>\n",
       "      <td>WA HYAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12143431</th>\n",
       "      <td>USC00453859</td>\n",
       "      <td>2019</td>\n",
       "      <td>SNWD</td>\n",
       "      <td>787</td>\n",
       "      <td>1302.157895</td>\n",
       "      <td>1499.0</td>\n",
       "      <td>1753</td>\n",
       "      <td>47.3872</td>\n",
       "      <td>-121.3925</td>\n",
       "      <td>776.9</td>\n",
       "      <td>WA HYAK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12143432 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           station_id  year  stat  value_min   value_mean  value_median   \n",
       "0         ITE00100554  1763  TMAX        -39   147.873973         147.0  \\\n",
       "1         ITE00100554  1763  TMIN        -63   100.657534          99.0   \n",
       "2         ITE00100554  1764  TMAX         20   153.592896         148.5   \n",
       "3         ITE00100554  1764  TMIN        -19   106.581967          93.0   \n",
       "4         ITE00100554  1765  TMAX        -14   149.994521         157.0   \n",
       "...               ...   ...   ...        ...          ...           ...   \n",
       "12143427  USC00453859  2019  DAPR          3     3.250000           3.0   \n",
       "12143428  USC00453859  2019  MDPR         38   189.250000         212.0   \n",
       "12143429  USC00453859  2019  PRCP          0    59.000000           3.0   \n",
       "12143430  USC00453859  2019  SNOW          0   103.526316          10.0   \n",
       "12143431  USC00453859  2019  SNWD        787  1302.157895        1499.0   \n",
       "\n",
       "          value_max  latitude  longitude  elevation     name  \n",
       "0               309   45.4717     9.1892      150.0    MILAN  \n",
       "1               233   45.4717     9.1892      150.0    MILAN  \n",
       "2               301   45.4717     9.1892      150.0    MILAN  \n",
       "3               236   45.4717     9.1892      150.0    MILAN  \n",
       "4               292   45.4717     9.1892      150.0    MILAN  \n",
       "...             ...       ...        ...        ...      ...  \n",
       "12143427          4   47.3872  -121.3925      776.9  WA HYAK  \n",
       "12143428        295   47.3872  -121.3925      776.9  WA HYAK  \n",
       "12143429        516   47.3872  -121.3925      776.9  WA HYAK  \n",
       "12143430        711   47.3872  -121.3925      776.9  WA HYAK  \n",
       "12143431       1753   47.3872  -121.3925      776.9  WA HYAK  \n",
       "\n",
       "[12143432 rows x 11 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
