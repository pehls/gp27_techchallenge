{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os, sys\n",
    "\n",
    "def test_dir(dir):\n",
    "    if not(os.path.isdir(dir)):\n",
    "        os.mkdir(dir)\n",
    "    return os.path.isdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.gitkeep', 'noaa_global', 'tech_challenge', 'temp_change', 'wbpy']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('..//data//raw')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data from tech challenge\n",
    "\n",
    "- [Dados da Vitivinicultura](http://vitibrasil.cnpuv.embrapa.br/index.php?opcao=opt_01) was indicated as main source of data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw > interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando dataframe das exportações\n",
    "\n",
    "#0. Create folder\n",
    "test_dir('..//data//interim//tech_challenge//')\n",
    "\n",
    "df_exp = pd.read_csv('..//data//raw//tech_challenge//ExpVinho.csv', sep=';')\n",
    "df_exp = df_exp.melt(id_vars=['Id','País'])\n",
    "\n",
    "vars = df_exp.variable.unique()\n",
    "valor = [x for x in vars if x.endswith('.1')]\n",
    "quantidade = list(set(vars) - set(valor))\n",
    "\n",
    "df_quant = df_exp.loc[df_exp['variable'].isin(quantidade)]\\\n",
    "    .rename(columns={\n",
    "          'value':'quantidade_exportada_pais'\n",
    "        , 'variable':'ano'\n",
    "        , 'País':'pais'\n",
    "        , 'Id':'id'\n",
    "        })\n",
    "\n",
    "df_quant['ano'] = df_quant['ano'].astype(int)\n",
    "\n",
    "df_value = df_exp.loc[df_exp['variable'].isin(valor)]\\\n",
    "    .rename(columns={\n",
    "          'value':'valor_exportado_pais'\n",
    "        , 'variable':'ano'\n",
    "        , 'País':'pais'\n",
    "        , 'Id':'id'\n",
    "        })\n",
    "df_value['ano'] = [int(x.replace('.1', '')) for x in df_value['ano']]\n",
    "\n",
    "df_exp = pd.merge(df_quant, df_value, on=['id','pais','ano'])[['id','pais','ano','quantidade_exportada_pais','valor_exportado_pais']]\\\n",
    "    .drop(columns=['id'])\n",
    "\n",
    "\n",
    "df_exp.to_csv('..//data//interim//tech_challenge//exportacao_vinhos.csv',index=False, sep=';', decimal=',') # export data to share with the project group members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando df das comercializações no RS\n",
    "\n",
    "#0. Create folder\n",
    "test_dir('..//data//interim//tech_challenge//')\n",
    "\n",
    "df_com = pd.read_csv('..//data//raw//tech_challenge//Comercio.csv', sep=';', header=None)\n",
    "lista_anos = list(df_exp['ano'].unique())\n",
    "df_com.columns = ['id','id_produto','produto'] + lista_anos\n",
    "\n",
    "df_com = df_com\\\n",
    "    .melt(id_vars=['id','id_produto','produto'])\\\n",
    "    .rename(columns={\n",
    "          'variable':'ano'\n",
    "        , 'value':'quantidade_com_rs'\n",
    "    })\\\n",
    "    .drop(columns=['id'])\n",
    "\n",
    "df_com['id_produto'] = [str(x).strip().lower() for x in df_com['id_produto']]\n",
    "df_com['produto'] = [str(x).strip().lower() for x in df_com['produto']]\n",
    "df_com['ano'] = df_com['ano'].astype(int)\n",
    "\n",
    "df_com.to_csv('..//data//interim//tech_challenge//comercio_vinhos_rs.csv',index=False, sep=';', decimal=',') # export data to share with the project group members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando df das produções no RS\n",
    "\n",
    "#0. Create folder\n",
    "test_dir('..//data//interim//tech_challenge//')\n",
    "\n",
    "df_prod = pd.read_csv('..//data//raw//tech_challenge//Producao.csv', sep=';', header=None)\n",
    "lista_anos = list(df_exp['ano'].unique())\n",
    "df_prod.columns = ['id','id_produto','produto'] + lista_anos\n",
    "\n",
    "df_prod = df_prod\\\n",
    "    .melt(id_vars=['id','id_produto','produto'])\\\n",
    "    .rename(columns={\n",
    "          'variable':'ano'\n",
    "        , 'value':'quantidade_prod_rs'\n",
    "    })\\\n",
    "    .drop(columns=['id'])\n",
    "\n",
    "df_prod['id_produto'] = [str(x).strip().lower() for x in df_prod['id_produto']]\n",
    "df_prod['produto'] = [str(x).strip().lower() for x in df_prod['produto']]\n",
    "df_prod['ano'] = df_prod['ano'].astype(int)\n",
    "\n",
    "df_prod.to_csv('..//data//interim//tech_challenge//producao_vinhos_rs.csv',index=False, sep=';', decimal=',') # export data to share with the project group members"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interim > processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interim to processed\n",
    "\n",
    "#0. Create folder\n",
    "test_dir('..//data//processed//tech_challenge//')\n",
    "\n",
    "df_exp = pd.read_csv('..//data//interim//tech_challenge//exportacao_vinhos.csv', sep=';', decimal=',')\n",
    "df_prod = pd.read_csv('..//data//interim//tech_challenge//producao_vinhos_rs.csv', sep=';', decimal=',')\n",
    "df_com = pd.read_csv('..//data//interim//tech_challenge//comercio_vinhos_rs.csv', sep=';', decimal=',')\n",
    "\n",
    "df_prod_com = df_prod.merge(df_com, on=['id_produto','produto','ano'], how='outer')\n",
    "\n",
    "df_final = df_exp.merge(df_prod_com, on='ano', how='outer')\n",
    "\n",
    "df_final.to_csv('..//data//processed//tech_challenge//df_vinhos.csv',index=False, sep=';', decimal=',') # export data to share with the project group members"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Temperature Change over years](https://www.kaggle.com/datasets/sevgisarac/temperature-change)\n",
    "\n",
    "### raw > processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar output da mudança de temperatura\n",
    "\n",
    "#0. Create folder\n",
    "test_dir('..//data//processed//temp_change//')\n",
    "\n",
    "df = pd.read_csv(\"..//data//raw//temp_change//Environment_Temperature_change_E_All_Data_NOFLAG.csv\", encoding='latin-1') # csv file is encoding as latin-1 type\n",
    "df_countrycode=pd.read_csv('..//data//raw//temp_change//FAOSTAT_data_11-24-2020.csv') #this csv file includes ISO-3 Country Code, this mentioned in Data Wrangling \n",
    "\n",
    "#1. Renaming\n",
    "df.rename(columns = {'Area':'Country Name'},inplace = True)\n",
    "df.set_index('Months', inplace=True)\n",
    "df.rename({'Dec\\x96Jan\\x96Feb': 'Winter', 'Mar\\x96Apr\\x96May': 'Spring', 'Jun\\x96Jul\\x96Aug':'Summer','Sep\\x96Oct\\x96Nov':'Fall'}, axis='index',inplace = True)\n",
    "df.reset_index(inplace = True)\n",
    "\n",
    "#2. Filtering \n",
    "df = df[df['Element'] == 'Temperature change']\n",
    "\n",
    "#2. Drop unwanted columns from df_countrycode\n",
    "df_countrycode.drop(['Country Code','M49 Code','ISO2 Code','Start Year','End Year'],axis=1,inplace=True)\n",
    "df_countrycode.rename(columns = {'Country':'Country Name','ISO3 Code':'Country Code'},inplace=True)\n",
    "\n",
    "#3. Merging with df to df_country\n",
    "df = pd.merge(df, df_countrycode, how='outer', on='Country Name')\n",
    "\n",
    "#2. Drop unwanted columns\n",
    "df.drop(['Area Code','Months Code','Element Code','Element','Unit'],axis=1,inplace=True)\n",
    "\n",
    "#3.Channing dataframe organization\n",
    "df = df.melt(id_vars=[\"Country Code\", \"Country Name\",\"Months\",], var_name=\"year\", value_name=\"tem_change\")\n",
    "df[\"year\"] = [i.split(\"Y\")[-1] for i in df.year]\n",
    "\n",
    "df = df[df['Months']=='Meteorological year']# chose just year data\n",
    "df.drop(['Months'],axis=1,inplace=True) # dropped Months column\n",
    "df.to_csv('..//data//processed//temp_change//temperature_change_Data.csv',index=False, sep=';', decimal=',') # export data to share with the project group members"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NOAA global data](https://www.kaggle.com/datasets/noaa/noaa-global-historical-climatology-network-daily)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw > interim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "base_path = '..\\\\data\\\\raw\\\\noaa_global'\n",
    "dest_path = '..\\\\data\\\\interim\\\\noaa_global'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dir(dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_year(file_year='2019'):\n",
    "    base_path = '..\\\\data\\\\raw\\\\noaa_global'\n",
    "    dest_path = '..\\\\data\\\\interim\\\\noaa_global'\n",
    "    if not os.path.exists(f\"{dest_path}\\\\{file_year}.csv\"):\n",
    "\n",
    "        start = datetime.datetime.now()\n",
    "\n",
    "        raw_df = pd.read_csv(f\"{base_path}\\\\ghcnd_all_years\\\\{file_year}.csv.gz\",\n",
    "                        usecols=[0,1,2,3], \n",
    "                        names=['station_id','date', 'stat', 'value'], \n",
    "                        dtype= {\n",
    "                              'station_id' : str\n",
    "                            , 'date': str\n",
    "                            , 'stat': str\n",
    "                            , 'value': np.int16\n",
    "                            },\n",
    "                        engine='c'\n",
    "                        )\n",
    "        raw_df['year'] = np.int16(file_year)\n",
    "        grouped = raw_df.groupby([\"station_id\", 'year',\"stat\"]).mean().reset_index()\n",
    "        grouped = grouped[['year', 'station_id', 'stat', 'value']]\n",
    "        #return grouped\n",
    "        grouped.to_csv(f\"{dest_path}\\\\{file_year}.csv\", index=False)\n",
    "        duration = (datetime.datetime.now() - start).seconds\n",
    "        print(f\"{file_year} took {round(duration/60, 2)} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [year[:4] for year in os.listdir(f\"{base_path}\\\\ghcnd_all_years\\\\\")]\n",
    "years.sort()\n",
    "level_of_parallelism = mp.cpu_count()\n",
    "pool = mp.Pool(level_of_parallelism)\n",
    "pool.map(process_year, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations = pd.read_csv(base_path+'\\\\ghcnd-stations.txt', sep=';', decimal='.')\n",
    "for col in ['id','name']:\n",
    "    df_stations[col] = [x.strip() for x in df_stations[col]]\n",
    "df_stations.to_csv(dest_path+'\\\\df_stations.csv',index=False, sep=';', decimal=',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interim > processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '..\\\\data\\\\interim\\\\noaa_global\\\\'\n",
    "dest_path = '..\\\\data\\\\processed\\\\noaa_global\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [base_path + x for x in os.listdir(base_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: '..\\\\data\\\\interim\\\\noaa_global\\\\*'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_noaa \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(base_path\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      2\u001b[0m                       dtype\u001b[39m=\u001b[39;49m {\n\u001b[0;32m      3\u001b[0m                               \u001b[39m'\u001b[39;49m\u001b[39mstation_id\u001b[39;49m\u001b[39m'\u001b[39;49m : \u001b[39mstr\u001b[39;49m\n\u001b[0;32m      4\u001b[0m                             , \u001b[39m'\u001b[39;49m\u001b[39mdate\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mstr\u001b[39;49m\n\u001b[0;32m      5\u001b[0m                             , \u001b[39m'\u001b[39;49m\u001b[39mstat\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mstr\u001b[39;49m\n\u001b[0;32m      6\u001b[0m                             , \u001b[39m'\u001b[39;49m\u001b[39mvalue_min\u001b[39;49m\u001b[39m'\u001b[39;49m : np\u001b[39m.\u001b[39;49mfloat64\n\u001b[0;32m      7\u001b[0m                             , \u001b[39m'\u001b[39;49m\u001b[39mvalue_mean\u001b[39;49m\u001b[39m'\u001b[39;49m : np\u001b[39m.\u001b[39;49mfloat64\n\u001b[0;32m      8\u001b[0m                             , \u001b[39m'\u001b[39;49m\u001b[39mvalue_median\u001b[39;49m\u001b[39m'\u001b[39;49m : np\u001b[39m.\u001b[39;49mfloat64\n\u001b[0;32m      9\u001b[0m                             , \u001b[39m'\u001b[39;49m\u001b[39mvalue_max\u001b[39;49m\u001b[39m'\u001b[39;49m : np\u001b[39m.\u001b[39;49mfloat64\n\u001b[0;32m     10\u001b[0m                             },\n\u001b[0;32m     11\u001b[0m                         engine\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mc\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[0;32m     12\u001b[0m                       )\n",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32md:\\Cursos\\FIAP_pós\\gp27_techchallenge\\.venv\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: '..\\\\data\\\\interim\\\\noaa_global\\\\*'"
     ]
    }
   ],
   "source": [
    "df_noaa = pd.read_csv(base_path+\"*\",\n",
    "                      dtype= {\n",
    "                              'station_id' : str\n",
    "                            , 'date': str\n",
    "                            , 'stat': str\n",
    "                            , 'value_min' : np.float64\n",
    "                            , 'value_mean' : np.float64\n",
    "                            , 'value_median' : np.float64\n",
    "                            , 'value_max' : np.float64\n",
    "                            },\n",
    "                        engine='c'\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station_id</th>\n",
       "      <th>stat</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1763</td>\n",
       "      <td>ITE00100554</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>147.873973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1763</td>\n",
       "      <td>ITE00100554</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>100.657534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year   station_id  stat       value\n",
       "0  1763  ITE00100554  TMAX  147.873973\n",
       "1  1763  ITE00100554  TMIN  100.657534"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noaa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interim > processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "\n",
    "base_path = '..\\\\data\\\\interim\\\\noaa_global\\\\'\n",
    "\n",
    "def read_csv(args):\n",
    "    return pd.read_csv(args, sep=';', decimal='.')\n",
    "\n",
    "df_noaa = pd.concat(map(read_csv, glob.glob(base_path+'years\\\\*.csv')))\n",
    "\n",
    "df_stations = pd.read_csv(base_path+'df_stations.csv', sep=';', decimal=',').rename(columns={'id':'station_id'})\n",
    "\n",
    "df_noaa = df_noaa.merge(df_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_path = '..\\\\data\\\\processed\\\\noaa_global\\\\'\n",
    "df_noaa.to_csv(dest_path+'noaa_global.csv', index=False, sep=';', decimal=',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [WBPY Data](https://pypi.org/project/wbpy/)\n",
    "\n",
    "- Was made from https://documents.worldbank.org/en/publication/documents-reports/api\n",
    "- Indicators in http://api.worldbank.org/v2/indicator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw > processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "base_path = '..\\\\data\\\\raw\\\\wbpy\\\\'\n",
    "dest_path = '..\\\\data\\\\processed\\\\wbpy\\\\'\n",
    "test_dir(dest_path)\n",
    "def read_csv(args):\n",
    "    df = pd.read_csv(args, sep=',', decimal='.')\n",
    "    df['metric'] = args.split('\\\\')[-1].split('.')[0]\n",
    "    return df\n",
    "\n",
    "df_wbpy = pd.concat(map(read_csv, glob.glob(base_path+'*.csv'))).rename(columns={'Unnamed: 0':'year'})\n",
    "df_wbpy = df_wbpy.melt(id_vars=['year','metric']).rename(columns={'variable':'country'})\n",
    "df_wbpy.to_csv(dest_path+'wbpy.csv', index=False, sep=';', decimal=',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interim > processed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "for root, dirs, files in os.walk('..\\\\data\\\\raw', topdown=False):\n",
    "    for name in files:\n",
    "        if not ('ghcnd-stations.txt' in name):\n",
    "            os.remove(os.path.join(root, name))\n",
    "    for name in dirs:\n",
    "        try:\n",
    "            os.rmdir(os.path.join(root, name))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inmet = RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "base_path = r'..\\\\data\\\\processed'\n",
    "\n",
    "df_clima_rs =  pd.read_csv(r'..\\\\data\\\\processed\\\\inmet\\\\rs.csv', sep=';', decimal=',')\n",
    "df_clima_rs['data'] = pd.to_datetime(df_clima_rs['data'], format='mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -9999.0 was used as a replace for nan values, so we put nan in it again:\n",
    "df_clima_rs['temp_min'] = df_clima_rs['temp_min'].replace(-9999.0, np.nan)\n",
    "df_clima_rs['temp_max'] = df_clima_rs['temp_max'].replace(-9999.0, np.nan)\n",
    "df_clima_rs['prec'] = df_clima_rs['prec'].replace(-9999.0, np.nan)\n",
    "\n",
    "# first, calculate daily precipitation rating, and max-min of temp:\n",
    "df_clima_rs = df_clima_rs.groupby('data').agg({\n",
    "    'prec':'sum',\n",
    "    'temp_max':('mean','max'),\n",
    "    'temp_min':'min'\n",
    "}).reset_index()\n",
    "\n",
    "df_clima_rs.columns = ['data','PRCP','TAVG','TMAX','TMIN']\n",
    "\n",
    "# now, we calculate the same metrics per year, recovering the mean for each variable in the year (same as noaa data):\n",
    "\n",
    "df_clima_rs['year'] = df_clima_rs.data.dt.year\n",
    "\n",
    "df_clima_rs = df_clima_rs.groupby('year').agg({\n",
    "    'PRCP':'median',\n",
    "    'TAVG':'mean',\n",
    "    'TMAX':'mean',\n",
    "    'TMIN':'mean'\n",
    "}).reset_index()\n",
    "\n",
    "df_clima_rs.to_csv(r'..\\\\data\\\\processed\\\\inmet\\\\rs_final.csv', sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
